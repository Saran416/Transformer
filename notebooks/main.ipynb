{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a773c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from model.transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42cbab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9124c",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b862167",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../src/data/translation_train.csv\")\n",
    "\n",
    "test_data = pd.read_csv(\"../src/data/translation_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa6b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(seq, max_len, pad_token='#'):\n",
    "    return seq + [pad_token] * (max_len - len(seq))\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text.replace('\\xa0', '').replace('_', '').replace('\\t', '')\n",
    "    return re.sub(r'[^\\w\\s]', '', text).lower().strip()\n",
    "\n",
    "def tokenize(data):\n",
    "    new_data = []\n",
    "    eng_maxlen = 0\n",
    "    ger_maxlen = 0\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        eng = clean_text(row.iloc[0])\n",
    "        ger = clean_text(row.iloc[1])\n",
    "\n",
    "        eng_tokens = ['<'] + list(eng) + ['>']\n",
    "        ger_tokens = ['<'] + list(ger) + ['>']\n",
    "\n",
    "        eng_maxlen = max(eng_maxlen, len(eng_tokens))\n",
    "        ger_maxlen = max(ger_maxlen, len(ger_tokens))\n",
    "\n",
    "        new_data.append({\"en\": eng_tokens, \"ge\": ger_tokens})\n",
    "\n",
    "    # Apply padding\n",
    "    for it in new_data:\n",
    "        it['en'] = pad_sequences(it['en'], eng_maxlen)\n",
    "        it['ge'] = pad_sequences(it['ge'], ger_maxlen)\n",
    "\n",
    "    return new_data, eng_maxlen, ger_maxlen\n",
    "\n",
    "def get_vocab_separate(data):\n",
    "    en_vocab = {'#': 0, '<': 1, '>': 2}\n",
    "    ge_vocab = {'#': 0, '<': 1, '>': 2}\n",
    "\n",
    "    en_index = 3\n",
    "    ge_index = 3\n",
    "\n",
    "    for it in data:\n",
    "        for tok in it['en']:\n",
    "            if tok not in en_vocab:\n",
    "                en_vocab[tok] = en_index\n",
    "                en_index += 1\n",
    "        for tok in it['ge']:\n",
    "            if tok not in ge_vocab:\n",
    "                ge_vocab[tok] = ge_index\n",
    "                ge_index += 1\n",
    "\n",
    "    return en_vocab, ge_vocab\n",
    "\n",
    "\n",
    "def embed_tokens(data, ge_vocab_dict, en_vocab_dict):\n",
    "    new_data = []\n",
    "    for it in data:\n",
    "        en_tokens = [en_vocab_dict.get(tok) for tok in it['en']]\n",
    "        ge_tokens = [ge_vocab_dict.get(tok) for tok in it['ge']]\n",
    "        new_data.append({'en': en_tokens, 'ge': ge_tokens})\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6812ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 40\n",
      "German Vocabulary Size: 48\n",
      "Max English Sequence Length: 203\n",
      "Max German Sequence Length: 249\n"
     ]
    }
   ],
   "source": [
    "tokenized_data, eng_maxlen, ger_maxlen = tokenize(train_data)\n",
    "en_vocab_dict, ge_vocab_dict = get_vocab_separate(tokenized_data)\n",
    "embedded_data = embed_tokens(tokenized_data, ge_vocab_dict, en_vocab_dict)\n",
    "\n",
    "print(\"English Vocabulary Size:\", len(en_vocab_dict))\n",
    "print(\"German Vocabulary Size:\", len(ge_vocab_dict))\n",
    "print(\"Max English Sequence Length:\", eng_maxlen)\n",
    "print(\"Max German Sequence Length:\", ger_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e677210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return torch.tensor(item['en']), torch.tensor(item['ge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfc3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharTranslationDataset(embedded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb9e92",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c81b2db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    encoder_vocab_size=len(en_vocab_dict),\n",
    "    decoder_vocab_size=len(ge_vocab_dict),\n",
    "    embed_dim=32,\n",
    "    num_heads=4,\n",
    "    ff_hidden_dim=64,\n",
    "    num_layers=3,\n",
    "    max_len=max(eng_maxlen, ger_maxlen)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725fc37b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad412012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, batch_size=32, epochs=10, lr=1e-4, device='cpu'):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0) \n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for src, tgt in loop:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            dec_input = tgt[:, :-1]\n",
    "            target = tgt[:, 1:]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(src, dec_input)\n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaec565",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train(model, dataset, batch_size=32, epochs=1, lr=1e-4, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f268f404",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1628250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence, en_vocab_dict, ge_vocab_dict, device='cpu', max_len=50):\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        tokens = ['<'] + list(sentence) + ['>']\n",
    "        token_ids = [en_vocab_dict.get(tok, 0) for tok in tokens]\n",
    "        src_tensor = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "\n",
    "        src_embed = model.encoder_embedding(src_tensor)\n",
    "        src_embed = model.encoder_pos(src_embed)\n",
    "        enc_out = src_embed\n",
    "        for layer in model.encoder_layers:\n",
    "            enc_out = layer(enc_out)\n",
    "\n",
    "        dec_input = torch.tensor([[ge_vocab_dict['<']]]).to(device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            tgt_embed = model.decoder_embedding(dec_input)\n",
    "            tgt_embed = model.decoder_pos(tgt_embed)\n",
    "\n",
    "            dec_out = tgt_embed\n",
    "            for layer in model.decoder_layers:\n",
    "                dec_out = layer(dec_out, enc_out)\n",
    "\n",
    "            logits = model.output_projection(dec_out)\n",
    "            next_token = logits[0, -1].argmax(-1).item()\n",
    "\n",
    "            dec_input = torch.cat([dec_input, torch.tensor([[next_token]]).to(device)], dim=1)\n",
    "\n",
    "            if next_token == ge_vocab_dict['>']:\n",
    "                break\n",
    "\n",
    "        output_tokens = dec_input.squeeze().tolist() \n",
    "        idx2word = {v: k for k, v in ge_vocab_dict.items()}\n",
    "        translated = ''.join([idx2word[tok] for tok in output_tokens[1:-1]])\n",
    "\n",
    "    return translated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
